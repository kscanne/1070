
In this lab we will explore a number of different data sources and
data formats that will be useful in the course going forward.

Some of this material is covered in Chapter 9 of the book 
("Getting Data"), which you should read some time this week.

(1) In lecture, I showed you how to download a plain text (.txt)
file using the command-line tool wget, and how to count the number
of times certain words appeared in the file using 
cat, egrep, and wc.

We did this all without the help of Python susing Linux command-line
tools.  The general philosophy here is more important than the
particular details of what we did; namely, the philosophy that
we can create a "data stream" from a file lusing "cat", and then
"pipe" that stream through small programs that process it
in some useful way, and pass the stream on to the next program in
the pipeline.

This is all well and good.  But the real power comes from the
fact that *we can create our own programs*, using Python, that
read an input stream and output a modified stream.  These can be
used just like any of the existing commanod-line tools we've seen, 
like egrep or wc.

In this directory, you'll see Python programs egrep.py and wc.py.
These are stripped-down versions of the command-line tools egrep and
wc (really "wc -l") written in Python.  Study the code carefully -
these are models for how you might write your own pipeline programs.

Then, try and reproduce the results from last lecture;
just run the python scripts in place of the command-line tools

$ wget http://monolith.sourceforge.net/genesis.txt
$ cat genesis.txt | python egrep.py 'Adam' | python wc.py 

etc.


(2) In the previous exercise we counted the number of lines
containing a given word (or really any pattern).  Let's take this to
the next level by writing a program that counts *all* of the words in
a text file.  This code is in wordcounts.py which is another program
intended to work in a pipeline; you'll see the same loop 
"for line in sys.stdin:" that you saw in egrep.py and wc.py.
This time we read a plain text file on input
and output a frequency table of all the words in the file.

To experiment with it, we'll want some new text files to play with.

For example, the Complete Works of Shakespeare from Project Gutenberg:

$ wget http://www.gutenberg.org/files/100/100-0.txt

and Ulysses by James Joyce:

$ wget http://www.gutenberg.org/files/4300/4300-0.txt

$ cat pg100.txt | python wordcounts.py

To slow down the output, you can pipe through the tool "more":

$ cat pg100.txt | python wordcounts.py | more

(space to page down, q to quit)

Generate frequency lists from these two files and compare the 
most frequent words in each.

First, note that we haven't included apostrophes or hyphens
as legal characters in words in the program wordcounts.py.
Can you add those to the script so that we count contractions, etc.
as legal words?

Next, do you think you could use these frequency lists to do
author identification?  Do you see particular words that are 
very characteristic of Shakespeare vs. Joyce, or vice versa?
Is something like "Bloom" (the main character in Ulysses) 
characteristic of James Joyce or an accident of the one book
we chose as "training data"?

(3) In last week's lab, the program netflix.py read in a 
comma-separated values (CSV) file containing the movie rating data.
That was relatively easy since we only had numbers in the file.
But CSV files are used to store tabular data of many different kinds - 
anything you might see in a spreadsheet. Reading these can be tricky
if the fields are strings which might themselves contain commas!

Fortunately, Python has tools for making it easy to read in CSV files.

I've given you a CSV file tweets.csv that contains data from Twitter,
showing the number of tweets (filtered in various ways) coming from
a given location specified as a latitude and longitude.

Study the program readcsv.py to see how we can read this data in and
access the three fields for any given row.

Ireland is contained (more or less) in a bounding box between
latitudes +51.42 and +55.44, and between longitudes -5.42 to -10.67.
Modify the script so that it prints out only the rows
for which the (latitude,longitude) is in Ireland. You can replace
the condition "if count > 0:" with an appropriate condition involving
the latitudes and longitudes (Python allows you to combine conditions
with the keywords "and" and "or" as needed).

(4) Finally, we'll "scrape" some useful information from an HTML file.
In any Wikipedia article, you'll see links in the left-hand column to
the same topic in other languages.  This is a quick and easy way of
building a massive multilingual dictionary, and for analysing the
way different languages create technical terms, which is one of 
my research interests.

Open the program wiki.py in this directory and study the code.
This program uses a library called "Beautiful Soup" for parsing HTML files.
Beautiful Soup provides the ability to extract information from particular
sections or tags in an HTML document.  Usually, you'll look at the HTML
for a particular web page you're interested in scraping to see how the
information is structured, and then write the Python program to
extract exactly what you want.

In this case, the program finds all <a> tags (links) that have
an attribute "hreflang" (the language code of the linked document).
It then prints out the "title" attribute of these links, which shows
the given term in each language, along with the name of the language.

Run it like this:

$ python wiki.py 'Cosmology'
$ python wiki.py 'Barack_Obama'

It's worth looking at the HTML itself to see how I came up with this.
For example, you can download the HTML for the Wikipedia article on
Cosmology using wget:

$ wget https://en.wikipedia.org/wiki/Cosmology

Then open it in your favorite text editor and search for the 
links to other languages (you could search for "Italiano" for example).
You'll see these are the only links that have an "hreflang" attribute,
and that the title attribute contains the term we want.
