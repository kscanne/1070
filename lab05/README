
In this lab we will use hierarchical clustering to try and 
reproduce the phylogenetic tree of (many) Indo-European languages.

From the discussion in class, you know that we need:
(a) a model that allows us to "vectorize" each language
(b) a choice of distance function between vectorized languages

For (a), we want a vectorization that is both easy to compute and
which will allow for easy comparison between languages.  Using words
as features won't really do, since it's unusual for many full words
to match across even relatively close languages (French and Italian,
say). Instead, we'll use what are called "character trigrams" as
features.  These are just all of the three-letter sequences in the
languages; for example, the English word "computer" consists of the
character trigrams "com", "omp", "mpu", "put", "ute", "ter".
Here we're relying on the fact that similar languages often
(but not always!) have similar writing systems, so even if the
full words don't match up, many character trigrams will. 

The choice of distance function is tricky any time we use 
counts (of words, trigrams, or whatever) to compute our
vectorization.  The problem is that we might have different
amounts of training data for each different language we're
considering (in fact, you might recall this was the case for the
lab where we looked at dialects of English).  When this happens,
the Euclidean distance between two vectorized languages might
be very different, but only because the *magnitudes* (lengths)
of the vectors are very different.  So Euclidean distance and in 
fact all of the Lp distances won't help.  A common solution 
is to use the *angle* between the vectors as a distance
measure.  The code for this lab uses what's called the
"Pearson correlation", which we'll discuss in class next week.


The dataset for the lab is contained in the file IE.txt.
If you open it in your favorite text editor, you'll see that it's
a simple tab-separated values file; as usual for us, the rows are
the objects we're classifying/clustering (languages), and the columns
are the features (character trigrams). The numerical entry in a given
row/column is the number of times the trigram corresponding to that 
column appears in a corpus of text in the language corresponding to the row.
The first row is a "header row" that gives the trigrams themselves.
The first column contains a unique language code for each language.
There are 4152 columns in total, and 145 rows (so 144 languages).

The code that performs the hierarchical clustering is contained in the
file clusters.py.  You're welcome to open that file in a text editor
to get the lay of the land.

******************************************************
**** For those of you running Python a Mac laptop ****
If you're just using ssh to connect to hopper, you don't need to
do anything special.  But if you're doing all of the work on your
laptop, you'll need to install a Python library that's
probably not already on your computer.
To install it:
$ sudo easy_install pip
$ sudo pip install Pillow

**** If you're running Ubuntu Linux on your laptop ****
Same deal, if you're doing all of the work on hopper via ssh,
then you don't need to do anything special.  If you need to 
install the library on your laptop, you can do so 
with this command:
$ sudo apt install python-pip
**************************************************

We'll actually perform the clustering
at the Python prompt, so start the interpreter in the usual way:

$ python

Then import the code from clusters.py as follows:

>>> import clusters

(the ">>>" is the Python prompt, so you don't type that)
If you get an error about a missing module PIL, see the
installation instructions above.  PIL should already be on hopper.

Then load the dataset:

>>> languages,trigrams,data=clusters.readfile('IE.txt')

This gives you the list of languages in case you need it:

>>> print languages

And perform the clustering itself (this will take a couple of minutes):

>>> clust=clusters.hcluster(data)

You can print the tree to the screen as follows:

>>> clusters.printclust(clust,labels=languages)

Although this is a bit hard to read.  It's also possible to 
generate a JPEG image of the tree:

>>> clusters.drawdendrogram(clust,languages,jpeg='indoeuropean.jpg')

This will save the image 'indoeuropean.jpg' in the lab05 folder.  You 
can navigate to it in the Desktop file browser and click to open it
in a suitable viewer.

Once you've generated the tree, you should compare it with the
known linguistic classification of Indo-European languages.
Wikipedia (https://en.wikipedia.org/wiki/Indo-European_languages) 
is a good starting point.

(1) Which of the major families (Baltic, Slavic, Germanic,
Romance, etc.) are recognizable in the tree you generated?  
Try to identify a single branch point in the tree such that all
(or nearly all) of the languages "beneath" that point are in the 
family.

(2) Celtic is interesting; linguistically we know there are two subfamilies,
P-Celtic (Welsh, Breton, and Cornish), and Q-Celtic (Irish, Scottish 
Gaelic, and Manx Gaelic).  Where did these six languages end up in your 
tree?  Are the two subfamilies reproduced in the tree?

(3) Examine some of the language pairs that are immediate neighbors...
if you remember the algorithm, most of these were joined early in the
process, so they're among the closest pairs in the whole dataset. 
Most of these should be closely-related languages (cf the varieties
of Albanian, Romani, or Serbian/Bosnian/Croatian)..

(4) Can you find any "outliers"?  By this I mean languages that are
clustered by the algorithm into the "wrong" linguistic family? 
Remember that our model only takes the writing system into account,
and that in an extremely naive way.

(5) What about English? It's strictly speaking a Germanic language,
but has borrowed heavily from the Romance languages. Do you think
these facts help explain its position in the computed tree?
