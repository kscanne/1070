
In this lab we'll train a number of n-gram language models
and use them in a couple of different applications.

A recurring theme in this course has been that models
based on word frequencies can be very sensitive to
the texts we use to train them.  We'll see this very
concretely in the examples below!


The code for this lab is in the file ngrams.py.  Start
by opening that file in your favorite text editor and
having a look around.

Next, you'll want to collect some text you can use to
train the language model.  For example, you could
download Romeo and Juliet by Williams Shakespeare from
Project Gutenberg: 

$ wget http://www.gutenberg.org/cache/epub/1112/pg1112.txt

It's probably a good idea to open this file in a text editor
and delete the "boilerplate" text from the beginning and
the end of the file so that it just contains the text of the play.

You might also spend a few minutes collecting some
additional texts for comparison purposes.  You can
browse Project Gutenberg and download books by other 
authors, perhaps with different writing styles or from
different genres, for example.

One amusing example is to use Donald Trump's political 
speeches as training data.  Someone has collected these
and saved them in a github repository:

$ git clone https://github.com/ryanmcdermott/trump-speeches.git

We'll work at the Python interpreter:

$ python
>>> from ngrams import *

Now read one of your text files to start building the language model.
For Shakespeare:

>>> tokens_ws = read_corpus('pg1112.txt')

or for Trump's speeches:

>>> tokens_dt = read_corpus('trump-speeches/speeches.txt')

and so on, for whatever files you're interested in.

This function reads the file, breaks it into "tokens" 
(words, punctuation, etc.) and returns these in a kind 
of list.  You can see the 664th word in the Romeo and Juliet file
as follows:

>>> print tokens_ws[663]

and so on.

(1) Now let's build the language model.  Recall that an n-gram 
language model gives the conditional probability of any word,
conditioned on the (n-1) previous words. To train a 2-gram
model from Romeo and Juliet, do this:

>>> ws2 = train_model(tokens_ws,2)

This returns a pretty interesting data structure. Since n=2,
each probability is conditioned on the one preceding word. 
Try entering this (exactly as it appears, including the weird trailing
comma):

>>> ws2[('sweet',)]

What you'll see is a list of all of the words that follow "sweet"
in the text, along with the number of times they followed "sweet".

Try a few others: "romeo", "juliet", "the", etc.

Now train a 3-gram model:

>>> ws3 = train_model(tokens_ws,3)

Here you'll need to provide a *pair* of words, and you'll see a 
list of all of the words that follow that pair in the play
along with the counts:

>>> ws3[('thy','love',)]
{u'did': 1, u'prove': 1, u',': 4, u'.': 2}

Note that we count punctuation marks just like actual words.

You can even build a 1-gram model!

>>> ws1 = train_model(tokens_ws,1)

Effectively all this is doing is remembering the frequency of
each word seen in training, with no conditioning on prior words.

Try building models for different values of n, and for different
training texts, and save them with mnemonic names (like ws1, ws2, etc.)
for the next steps.

(2) To this point, we've thought of a language model as a way
to assign probabilities to sentences and phrases, but a language
model can also be used "generatively", which is to say we can
use it to generate "random" text, not really randomly, but
following the word probabilities that we learned from the training
data.  

So for example, if you use the 3-gram model ws3 trained above on
Romeo and Juliet, and you start a text with "thy love", then 
we would choose the next token from the four seen in training
according to their conditional probabilities:

P(did | thy love) = 1/8 = 0.125
P(, | thy love) = 4/8 = 0.5
P(. | thy love) = 2/8 = 0.25
P(prove | thy love) = 1/8 = 0.125

In this case, since "thy love" appeared eight times total in training,
it's a bit like we're rolling an 8-sided die.  If we roll a 1,
then we'd output "did" as the next token, if we roll 2,3,4 or 5, then
output a comma, etc.  Let's say we generate "did" as the next word.  Now
the last two words of the generated text are "love did", and we repeat the
same process as many times as we want.

To generate 250 words of text in this way using the model ws3, use
this command:

>>> print generate(ws3, 250)

You can repeat this same command and you'll get different text every time.
(Spammers use this technique to generate fake "content" to 
attract search engine traffic...)

Now generate text using a 2-gram model, and compare the output
with what you got from the 3-gram model.  Does it seem more or less
"grammatical"?  Then try a 1-gram model.

As always, you should think about the extreme case!  What if you train
a 10-gram model and generate a text using that?  Try and guess what
will happen, and then test your prediction.

This is an example of a model with way too many parameters!
Effectively all we've done is to "memorize" the training data
that we saw; there is no real learning going on.

(3) With only a slight tweak, we can turn this into a 
predictive text engine!  In this case, once a user has input (n-1) words,
we can use the model to suggest to the user the words that followed 
these (n-1) words most often in the training texts.

You can try this out using the Romeo and Juliet 3-gram model as follows
(here 15 is the number of words you want to type):

>>> predictive_text(ws3,15)

You'd probably be very unhappy if this were the predictive text
on your phone!

Let's try retraining on a collection of actual text messages
and see if we end up any better.  In a separate window, do the following:

$ wget https://web.archive.org/web/20130921042957/http://cybersecurity.cit.purduecal.edu:80/content/dl/master_corpus.txt

This file has some metadata we don't want, so use this magic command
to extract out just the text messages into a new file sms.txt:

$ cat master_corpus.txt | tr "\015" "\n" | iconv -f iso-8859-1 -t utf8 | cut -f 3 | sed 1d > sms.txt 

Then, back in the window running Python:

>>> tokens_sms = read_corpus('sms.txt')
>>> sms3 = train_model(tokens_sms,3)
>>> predictive_text(sms3,15)


(4) This exercise is to illustrate that the code in ngrams.py
is (more or less) language independent.  For some other language
you know, download as much text as you can (Project Gutenberg
has books in other languages, or you may know of other good
sources) and save that in a .txt file in this directory.

The only change you should need to make to ngrams.py is
in the "read_corpus" function, where we have an expression
that gives the set of legal alphabetic letters in the language
of interest.  For English this is "[a-z’'-]".  For your language,
add any special characters that are found as part of words.
In Irish, for example, you'd want to use "[a-záéíóú’'-]"

(Note: Chinese and Japanese are difficult since it's tricky to list
all of the acceptable "word characters".  Thai, Khmer, Tibetan, etc.
are difficult since "words" aren't separated by spaces. Most
other languages should work, in principle...)

Create n-gram language models for n=1,2,3, and generate text
using these models.  How "grammatical" is the resulting text?
Do the results seem better or worse than for English with the
same n-value?
